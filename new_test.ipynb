{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d7f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pathlib as p\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "from monai.networks.nets import UNet\n",
    "from monai.metrics import PSNRMetric, SSIMMetric, RMSEMetric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import mean\n",
    "import pickle\n",
    "from model import UnetGenerator3D\n",
    "from dataset import TrainDataset\n",
    "from preprocessing import normalize, reconstruct_from_patches, split_dataset, extract_3D_patches, get_patches, pad_to_shape, min_max_normalize\n",
    "from file_structure import append_row\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69ac8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = p.Path.home()/\"data\"/\"bobsrepository\"\n",
    "t1_files = sorted(DATA_DIR.rglob(\"*T1w.nii.gz\"))\n",
    "t2_files = sorted(DATA_DIR.rglob(\"*T2w.nii.gz\"))\n",
    "t2_LR_files = sorted(DATA_DIR.rglob(\"*T2w_LR.nii.gz\"))\n",
    "\n",
    "#t2_LR_files = create_and_save_LR_imgs(t2_files, scale_factor=2, output_dir=DATA_DIR/\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001d2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = list(zip(t1_files, t2_files, t2_LR_files))\n",
    "\n",
    "#SPLIT DATASET\n",
    "train, val, test = split_dataset(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f9d5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#EXTRACT PATCHES\n",
    "\n",
    "patch_size = (64, 64, 64)\n",
    "stride = (32, 32, 32)\n",
    "ref_img = nib.load(str(t1_files[0]))\n",
    "target_shape = (192, 224, 192) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08326e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_t1, train_t2, train_t2_LR = get_patches(train, patch_size, stride, target_shape, ref_img)\n",
    "val_t1, val_t2, val_t2_LR = get_patches(val, patch_size, stride, target_shape, ref_img)\n",
    "test_t1, test_t2, test_t2_LR = get_patches(test, patch_size, stride, target_shape, ref_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b802c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0034\n",
      "Epoch 2/2, Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "#NETWORK TRAINING\n",
    "\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "# Flatten train data into a single list of patches\n",
    "input_1 = [patch for img_patches in train_t1 for patch in img_patches]\n",
    "input_2 = [patch for img_patches in train_t2_LR for patch in img_patches]\n",
    "output = [patch for img_patches in train_t2 for patch in img_patches]\n",
    "\n",
    "train_dataset = TrainDataset(input_1, input_2, output)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle)\n",
    "\n",
    "net = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=None,\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "num_epochs = 2\n",
    "device = torch.device(\"cpu\") \n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Unpack your batch\n",
    "        input1, input2, target = batch\n",
    "        # Stack inputs along channel dimension\n",
    "        inputs = torch.stack([input1, input2], dim=1).float().to(device)  # (B, 2, 64, 64, 64)\n",
    "        target = target.unsqueeze(1).float().to(device)  # (B, 1, 64, 64, 64)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dd59a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed validation image 1/11\n",
      "Processed validation image 2/11\n",
      "Processed validation image 3/11\n",
      "Processed validation image 4/11\n",
      "Processed validation image 5/11\n",
      "Processed validation image 6/11\n",
      "Processed validation image 7/11\n",
      "Processed validation image 8/11\n",
      "Processed validation image 9/11\n",
      "Processed validation image 10/11\n",
      "Processed validation image 11/11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net.eval()\n",
    "generated_images = []\n",
    "real_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_t1)):\n",
    "        all_outputs = []\n",
    "        for j in range(len(val_t1[0])):\n",
    "            input1 = torch.tensor(val_t1[i][j]).float()\n",
    "            input2 = torch.tensor(val_t2_LR[i][j]).float()\n",
    "            inputs = torch.stack([input1, input2], dim=0).unsqueeze(0)  # (1, 2, 64, 64, 64)\n",
    "            output = net(inputs)\n",
    "            all_outputs.append(output.squeeze(0).squeeze(0).cpu().numpy())  # (64, 64, 64)\n",
    "        gen_reconstructed = reconstruct_from_patches(all_outputs, target_shape, stride)\n",
    "        real_reconstructed = reconstruct_from_patches(val_t2[i], target_shape, stride)\n",
    "        generated_images.append(gen_reconstructed)\n",
    "        real_images.append(real_reconstructed)\n",
    "        print(f\"Processed validation image {i+1}/{len(val_t1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e6a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PSNRMetric(max_val=1.0, reduction=\"mean\")\n",
    "ssim = SSIMMetric(spatial_dims=3, data_range=1.0, kernel_type=\"gaussian\", win_size=11, kernel_sigma=1.5)\n",
    "\n",
    "gen_tensor = torch.tensor(np.stack(generated_images))  # shape: (B, H, W, D)\n",
    "real_tensor = torch.tensor(np.stack(real_images))      # shape: (B, H, W, D)\n",
    "\n",
    "psnr_value = psnr(gen_tensor, real_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba52589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 192, 224, 192])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add channel dimension\n",
    "gen_tensor = gen_tensor.unsqueeze(1)   # shape: (B, 1, H, W, D)\n",
    "real_tensor = real_tensor.unsqueeze(1) # shape: (B, 1, H, W, D)\n",
    "\n",
    "gen_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "036d0ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.7756, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "psnr_value = mean(psnr_value)\n",
    "print(psnr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e327864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "\n",
    "\n",
    "row_dict = {\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"train_size\": len(train),\n",
    "    \"val_size\": len(val),\n",
    "    \"test_size\": len(test),\n",
    "    \"patch_size\": patch_size,\n",
    "    \"stride\": stride,\n",
    "    \"target_shape\": target_shape,\n",
    "    \"normalization\": \"min-max\",\n",
    "    \"model\": \"MONAI 3D U-Net\",\n",
    "    \"net spatial_dims\": 3,\n",
    "    \"net in_channels\": 2,\n",
    "    \"net out_channels\": 1,\n",
    "    \"net channels\": (16, 32, 64, 128, 256),\n",
    "    \"net strides\": (2, 2, 2, 2),\n",
    "    \"net num_res_units\": 2,\n",
    "    \"net norm\": None,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"psnr\": psnr_value.item(), \n",
    "    \"ssim\": None,\n",
    "    \"lpips\": None,\n",
    "    \"rmse\": None,\n",
    "    \"loss_fn\": \"MSELoss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"notes\": \"Initial test run\",\n",
    "    \"masking\": \"None\",\n",
    "}\n",
    "\n",
    "append_row(DATA_DIR / \"results.csv\", row_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
