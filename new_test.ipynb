{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d7f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pathlib as p\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "from monai.networks.nets import UNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from model import UnetGenerator3D\n",
    "from dataset import TrainDataset\n",
    "from preprocessing import create_LR_img, scale_to_reference_img, pad_to_shape, extract_3D_patches, reconstruct_from_patches, split_dataset, create_and_save_LR_imgs, get_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526427ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = p.Path.home()/\"data\"/\"bobsrepository\"\n",
    "t1_files = sorted(DATA_DIR.rglob(\"*T1w.nii.gz\"))\n",
    "t2_files = sorted(DATA_DIR.rglob(\"*T2w.nii.gz\"))\n",
    "t2_LR_files = sorted(DATA_DIR.rglob(\"*T2w_LR.nii.gz\"))\n",
    "\n",
    "#t2_LR_files = create_and_save_LR_imgs(t2_files, scale_factor=2, output_dir=DATA_DIR/\"LR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001d2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = list(zip(t1_files, t2_files, t2_LR_files))\n",
    "\n",
    "#SPLIT DATASET\n",
    "train, val, test = split_dataset(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9d5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#EXTRACT PATCHES\n",
    "\n",
    "patch_size = (64, 64, 64)\n",
    "stride = (32, 32, 32)\n",
    "ref_img = nib.load(str(t1_files[0]))\n",
    "target_shape = (192, 224, 192) \n",
    "\n",
    "train_t1, train_t2, train_t2_LR = get_patches(train, patch_size, stride, target_shape, ref_img)\n",
    "val_t1, val_t2, val_t2_LR = get_patches(val, patch_size, stride, target_shape, ref_img)\n",
    "test_t1, test_t2, test_t2_LR = get_patches(test, patch_size, stride, target_shape, ref_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1_input = []\n",
    "t2_output = []\n",
    "t2_LR_input = []\n",
    "\n",
    "for t1_file, t2_file, t2_LR_file in train:\n",
    "    #scaling to reference image\n",
    "    t1_img = scale_to_reference_img(nib.load(t1_file), ref_img)\n",
    "    t2_img = scale_to_reference_img(nib.load(t2_file), ref_img)\n",
    "    t2_LR_img = scale_to_reference_img(nib.load(t2_LR_file), ref_img)\n",
    "    #padding to be divisible by patch size\n",
    "    t1_img = pad_to_shape(t1_img, target_shape)\n",
    "    t2_img = pad_to_shape(t2_img, target_shape)\n",
    "    t2_LR_img = pad_to_shape(t2_LR_img, target_shape)\n",
    "    #extracting patches\n",
    "    t1_patches = extract_3D_patches(t1_img.get_fdata(), patch_size, stride)\n",
    "    t2_patches = extract_3D_patches(t2_img.get_fdata(), patch_size, stride)\n",
    "    t2_LR_patches = extract_3D_patches(t2_LR_img.get_fdata(), patch_size, stride)\n",
    "    #saving patches\n",
    "    t1_input.append(t1_patches)\n",
    "    t2_output.append(t2_patches)\n",
    "    t2_LR_input.append(t2_LR_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9430430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "150\n",
      "(64, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_t1))  # Number of training samples\n",
    "print(len(train_t1[0]))  # Number of patches in the first training sample\n",
    "print(train_t1[0][0].shape)  # Shape of the first patch, should be (64, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NETWORK TRAINING\n",
    "\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "# Flatten train data into a single list of patches\n",
    "input_1 = [patch for img_patches in train_t1 for patch in img_patches]\n",
    "input_2 = [patch for img_patches in train_t2_LR for patch in img_patches]\n",
    "output = [patch for img_patches in train_t2 for patch in img_patches]\n",
    "\n",
    "train_dataset = TrainDataset(input_1, input_2, output)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle)\n",
    "\n",
    "net = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
